# ![CI logo](https://codeinstitute.s3.amazonaws.com/fullstack/ci_logo_small.png)

## About Dataset
The "Online Retail Transaction" dataset contains information on transactions made by customers through an online retail platform. The dataset includes data on the products that were purchased, the quantity of each product, the date and time of each transaction, the price of each product, the unique identifier for each customer who made a purchase, and the country where each customer is located. This dataset can be used to analyze customer behavior and preferences, identify popular products, and optimize pricing and marketing strategies. The dataset is well-suited for data analysis and machine learning applications, as it contains a large volume of transactional data that can be used to train predictive models and make data-driven decisions.

### Column Descriptors
- StockCode: A code used to identify the product that was purchased
- Description: A brief description of the product that was purchased
- Quantity: The quantity of the product that was purchased
- InvoiceDate: The date and time that the purchase was made
- UnitPrice: The price of one unit of the product that was purchased
- CustomerID: The unique identifier for the customer who made the purchase
- Country: The country where the customer who made the purchase is located

## Data analysis goals:

Analyse online retail transaction data to understand customer behaviour, identify popular products, and optimise pricing and marketing strategies. Provide insights into customer behaviour, popular products, and pricing strategies to improve sales and marketing efforts.

## Hypothesis and how to validate?
I believe there are many factors that affect the price of a car. Some are factors can be directly affected, like the size of an engine, or the number of doors, and other factors are fully/partially dependant like the length, width, power. From my vested interest in cars, not all of these factors are contributary to the price. Factors such as weight are not a selling point, and the price is listed for takes into consideration manufacturing and logistical costs among others.

The main factors I believe will affect the price are the brand, the horsepower, enginesize and economy. I will validate this by checking the correlation between the variables and price. An increase in horsepower and engine size should increase the price. Increasing these variables directly affects the mpg, and hence I predict the lower mpg cars will have a lower price
==
Hypotheses:

Most revenue is generated by a small segment of repeat customers (Pareto principle). Certain products contribute more to revenue due to seasonal trends. Discounts on popular products will increase overall revenue.

Validation Steps:

Identify high-value customers. Perform time series analysis to identify seasonal trends. Testing on discount strategies to measure revenue impact.
==
To identify the frequently bought products by creating a new column for the Revenue. The use to plot a barchart. To identify high spenders and frequent buyers using the customer segmentation created with the bar plot and pie chart. To identify spending habit by country using bthe merged dataframe.

## Project Plan
* Outline the high-level steps taken for the analysis.
Relevant test were carried to see which of the variables are significant in affecting the price

* How was the data managed throughout the collection, processing, analysis and interpretation steps?
Data was collected from kaggle. One of the first checks was to see if there was any missing data of which there was none. It was processed to make the distribution more closer to a normal distribution. Interpretation was done using my background knowledge in statistics
* Why did you choose the research methodologies you used?
Having studied statistics I knew the relevant methodologies to use. Since price was the main Variable I was interested, I had to check how and if the other variables contributed to it. 
===
Steps: Data Collection: Load the dataset from the provided source. Data Cleaning: Handle missing values, remove duplicates, and standardize formats. Data Transformation: Create new features, such as total transaction value. Analysis: Perform descriptive statistics, and trend analysis. Visualization: Build interactive dashboards to display insights.
===
Choosing the appropriate method to carry out the analysis from the start wwas important. I chose a method easy for me to understand and aslo to relay outcomes to the end user.
Loading data from the csv file downloaded from Kaggle onto a dataframe
Extracting and cleaning of the data by handling missing values,removing negative values from the quantity and price columns and also removing duplicates.
Tranforamation process involved creating a new column and adding it onto the dataframe .
Descriptive Analysis of the cleaned data to give us some correlations between the data cleaned. for example the RFM score.
The creation of visuals to further give insights into the exploratory analysis carried out.
Give a conclusion on what was derived from the analysis.

## The rationale to map the business requirements to the Data Visualisations
* List your business requirements and a rationale to map them to the Data Visualisations
Business requirements were to see what factors affected the price the most, the least or even at all. This information is extremely important as the right decisions need to be made before it reaches the manufacturing stage. Making changes after that is very costly
==
Product popularity analysis will be conducted and presented using a bar chart showing the top-selling products. This will assist in optimizing inventory and improving product offerings.

Sales trends will be visualized through a time series plot to help identify seasonal trends, allowing businesses to plan their marketing strategies accordingly.

Pricing strategies will be optimized by comparing revenue before and after applying discounts. This analysis will be shown through revenue comparison charts to demonstrate the impact of pricing changes on sales performance.
==
A barchart to show the Top products sold. This gives a straight forward insight into the most purchased products. A barplot or pie chart show the customer distribution, very simple insight into who the loyal buyers are. A heatmap is important in showingn the RFM analysis carried out

## Analysis techniques used
* List the data analysis methods used and explain limitations or alternative approaches.
Correlation analysis - doesn't always give the whole picture. Dependant on the size of the dataset
ANOVA test for categorical variables - assumes the variance of price is equal accross all the variables. It is sensitive to outliers

* How did you structure the data analysis techniques. Justify your response.
The aim was to remove as many features as I could that did not contribute enough as a factor to the price. I started with the correlation check to remove the features that had a weak correlation. This doesn't work on categorical variables so I used an ANOVA test on those and kept the variables with a p-value<0.05 
* Did the data limit you, and did you use an alternative approach to meet these challenges?
The data did limit me as the sample was too small. I couldn't perform an analysis to check if brand name affects the price which I believe it does. The sample size was also too small small to draw any meaningful information for features like aspiration which only had 19 cars with a turbo.

* How did you use generative AI tools to help with ideation, design thinking and code optimisation?
Gemini was used along the way to help me write the code, solve syntax problems, and brainstorm analysis technique.
===
Summarize key metrics like total sales, average transaction values, and customer count. Identify sales trends and seasonality over time.

Challenges and Solutions:

Missing values were handled using imputation techniques. Categorical variables were encoded for analysis. Seasonal patterns were extracted using moving averages and decomposition methods.

Use of AI Tools:

ChatGPT was used for brainstorming ideas and optimizing Python code.

===
. Initial RFM analysis to give relation between recency,frequency and revenue(monetary)

. Descriptive statistics in form of a bar chart showing the top 10 products purchased according to revenue.

. Used a Bar plot to show the Customer Segmentation,;highlighting Best Customer vs Low Value Customer. A pie chart was also used for this giving ban easier understanding to nthe customer distribution.

. A Heatmap was used to further give insight into the frequency and recency relative to revenue.

. A Boxplot time series was also used for further insight.

. A 3D Scatterplot was used to show the country of purchase in relation to customer behaviour.

## Ethical considerations
* Were there any data privacy, bias or fairness issues with the data?
Data was on a public server available to all so there was no issue with privacy. As mentioned before, sample size was very small and range of cars and classes was not big enough to make any conclusions based on those features
* How did you overcome any legal or societal issues?
I didn't not face any

Data Privacy: Customer IDs were anonymized to protect personal data. Bias and Fairness: The dataset was checked for sampling bias to ensure accurate analysis. Legal Compliance: The data was used within the permitted scope of the Kaggle license.


## Unfixed Bugs
There are no unfixed bugs in my code
* Did you recognise gaps in your knowledge, and how did you address them?
Gaps in my knowledge were filled by using resources such as the LMS for revision, and the internet
* If applicable, include evidence of feedback received (from peers or instructors) and how it improved your approach or understanding.
I had asked Vasi to have a quick glance to see if I was on the right track, which gave me confidence that I was doing the task correctly
===
Duplicate Transactions: There were a few duplicated transaction records that were not removed during data cleaning due to time constraints.

Geolocation Data: The dataset lacks complete geolocation information, limiting regional analysis. Slug Size Issue on Heroku: The dataset size exceeded Heroku's slug size limit, requiring a workaround using .slugignore.

Gaps and Solutions:

Identified gaps in time series forecasting, addressed through additional research and tutorials. Feedback from peers was used to improve dashboard visuals and code readability.

## Development Roadmap
* What challenges did you face, and what strategies were used to overcome these challenges?
* What new skills or tools do you plan to learn next based on your project experience? 
Challenges Faced:

Handling large dataset using ETL pipeline. Visualizing complex relationships between variables.

Next Steps:

Implement machine learning models to help analyse data.

## Main Data Analysis Libraries
Pandas: For data cleaning and manipulation. 
NumPy: For numerical computations. 
Matplotlib / Seaborn / Plotly: For visualizations.

## Kanban
[Project Planning] (https://github.com/users/enjoy15/projects/1)

## Data source 
[Kaggle] (https://www.kaggle.com/datasets/abhishekrp1517/online-retail-transactions-dataset )

## Acknowledgements (optional)
Vasi for the support

